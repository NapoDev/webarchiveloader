#DO THE FUCK YOU WANT COPYRIGH
#	AKA COPYLEFT
# Simple scrip that loads all given URLS in the url.txt (one per line) into web archive

from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
import os
import time

webarchive = "http://archive.org/web/"


textUrlId = "web_save_url" 
submitUrlId = "web_save_button"
file = "urls.txt"




#takes list of URLs exported from xenu, and sets them up to be imported to 
#web archive
def makeList(urlFile):
	listOfUrls = []
	urlsFile = open(urlFile, "r+")
	urls = urlsFile.readlines()
	for url in urls:
		if len(url) > 10:
			listOfUrls.append((url.replace("\n","")))
	return listOfUrls


def textSendToArchive(url, textId,submitId,firefox):
	try:
		firefox.get(webarchive)
		submit = firefox.find_element_by_id(submitId)
		textArea = firefox.find_element_by_id(textId)
		textArea.send_keys(url)
		submit.click()
		return True
	except:
		return False
	
def loadAllUrls():
	list = makeList(file)
	firefox = webdriver.Firefox()
	counter = 1
	for url in list:
		if(textSendToArchive(url,"web_save_url","web_save_button", firefox)):
			print(str(counter) + ": Added to webarchive")
			counter += 1
			time.sleep(15)
		elif(textSendToArchive(url,"web_save_url","web_save_button", firefox)):
			print(str(counter) + ": Added to webarchive")
			counter += 1
			time.sleep(15)
		else:
			print(str(counter) + ": Not to webarchive")
			counter += 1
loadAllUrls()
